Pacific-A

# ABSTRACE
设计一个易于practical replication protocl是门艺术。
这篇文章分享了设计和实践基于log的副本技术的经验。我们提出了一个简洁的，易于实践的，强一致性的通用的replication framework。这个framework可以灵活地支持不同的优化策略。实现了一个原型支持3种优化策略。

# 1. INTRODUCTION
基于普通服务器构建的分布式存储系统，failure是常态。通过副本技术事项可靠性和可用性。
经典的分布式一致性算法是Paxos。构建一个practical分布式系统并不仅仅实现一个副本协议那么简单。一个理论的副本协议和实践的副本系统存在巨大的鸿沟。比如，在设计一个副本协议时，通常性能模型过于简单：消息数。而一个practical的副本系统的目标是优化端到端的client看到的性能数据，比如throughtput，latency，recovery time。理论的复制协议聚焦单个instance，而一个practical的复制系统需要优化所有instance。
在这篇文章中，介绍了我们在设计，实现，评估一个基于log的存储系统的经验。基于的log的设计在存储系统中很常见，把随机写转化成顺序写，允许batch操作以支持事务。
基于log的存储系统提供了足够的弹性，支持有趣的设计选择，比如：复制过程可以在不同的语义层次完成(逻辑状态，日志，盘上的物理状态)。
我们选择，设计并实现一个simple，practical，provably的通用的复制框架。
在设计复制框架过程中，更加的坚定了一个经验：可被证明正确性的复制协议是practical复制系统的重要基石。一个好的系统模型需要对协议运行的环境建模，同时这个过程使得我们加深了对风险的认识。相反，一个临时的复制协议往往在某些cornor case下面是错误的。
选择一个合适的复制协议和架构，是实现一个包的复制框架的基础。简洁和模块化是构建practical系统的关键特性。我们的复制框架遵循如下原则：
	1）配置变更和data replicaton分离，依赖外部的Paxos管理变配和primary/backupde数据复制；
	2）去中心化的故障monitor，monitor数据复制的通信模式；
	3）一个被能够保证正确性的通用抽象的模型，支持其他的业务场景；
在log-base的分布式系统中，我们的复制框架允许探索和一直协议的如boxwood，bigtable不同的复制协议；比如，我们的协议不需要依赖外部的锁服务，因此能减少网络交互，以实现 数据更好的co-location。
我们实现了PacificA，它是一个log-based的分布式存储系统，支持存储结构化和半结构化的网页数据。我们借助PacificA来评估不同的复制模式的性能。

这篇文章的结构如下，第2章节阐述我们的提出的协议；第3章节阐述基于我们的协议可以实现不同的log-based的分布式系统；第4章节阐述PacificAde实现和性能；第5章节阐述相关工作。最后，第6章节做总结。

# 2. PacifiA Replication FrameWork
在这篇文章中，我们主要关注生产环境的分布式集群：局域网内部的大量服务器的系统。每个服务器通过本地告诉网络互连，并且无需考虑拜占庭错误。
我们假设是fail-stop模型，并且不假设消息延迟有一个明确的上限，也就是消息可以丢包乱序。不依赖每个机器的时钟。

数据被切分成多个片，每个分片在replica group中。一个节点可以服务不同的replica group，这是一个多对多的关系。数据复制协议是primary/backup模式。在一个replica group中，一个副本被指定为primary其他的为backup。configuration是指一个replica group的primary和secondaries分布。当一个replica出现故障或者添加了新的replica到一个replica group中，这个replica group的configuration就发生了变化，通过version来反应configuration的变化。

我们只关注强一致性的复制协议，因为强一致性最能代表一致性模型。非严格定义上来说，强一致性要求分布式系统的表现如同单机系统的语义一样：线性一致性。弱一致性模型会应用方增加复杂度。

## 2.1 Primary/Backup Data Replication
我们使用primary/backup的复制模式。client的请求分成：query类的和update类的。所有的请求都发给primary，primary在本地处理所有的请求，同时复制update请求给secondaries节点。primary/backup模式的优势是简单，primary做为处理请求的单节点和单机系统很接近。多个阶段的查询请求在primary单机上就能完成。

只要一个replica group中所有的节点都以相同的顺序处理client请求就能做到强一致性。primary连续地处理请求并给请求赋值连续的单调递增的序列号，同时secondaries也以相同的序列号处理请求。一个replica可以描述为prepared-list和committed-point。  <font color=red> prepared-list是按sn排序的请求, sn是连续的。 </font>   prepared-list的前缀一直到committed-point为committed-list。

顺序apply commited-list就能得到应用的状态机。只要不发生灾难性的故障(比如3副本都crash)，committed-list保证不丢失。prepared-list上的uncommitted(这个地方么有看懂)。定义committed-p和prepared-p为节点p的committed-list和prepared-list的请求集合。

Normal-Case Query and Update Protocol。在normal case中(比如没有发生reconfiguration)，replication的过程是很直观的。primary收到请求后，结合commit-list产生的状态机处理请求。

对一个update请求的处理流程是：
	1. primary给这个请求分配sn序列号；
	2. 构造prepared msg = sn + request + configuration version；
	3. primary广播prepared-msg；
	4. secondaries收到后插入到自己的prepared-list中，并恢复ack；
	5. primary等所有的replica都回复ack后，就可以commit这个消息了：
			a) 往前移动committed-point到最大位置，committed-point要求是连续的；
			b) 给client回复ack；
	6. primary在后续的prepared-msg中会把committed-point信心带给secondaries，这样secondaries也可以往前移动自己的commited-point;

primary的prepaed-list驱动secondaries的prepaed-list，secondaries的prepaed-list驱动primary的commtted-list，primary的committed-list驱动secondaries的committed-list；

Commit Invariant： p为primary，q为任意的replica，那么 committed-q \subset committed-p \subset prepared-q

在没有reconfiguration发生时，basic的primary/backup模式能够正常运行。我们的replication framework把配置管理和数据链路分离出来，下面将介绍配置管理和数据复制的交互。

## 2.2 Configuration Management
在我们的设计中，有一个管局配置管理器。负责维护所有replica group的分布。对于一个replica group来说，它维护这个group的configuration和version。

任何一个replica只要发现一个故障的replica(比如通过故障检测)，就可发起一个删除replica的reconfiguration过程，或者添加replica的reconfiguration过程(恢复指定的副本数目)。reconfiguration时，replica给配置管理发送新的配置和verison，配置管理校验version是否匹配，只有version匹配了才使用这个配置，同时增加version。

如果primary和secondaries出现了网络分区，primary和secondaries同时发起了reconfiguration：primary想移除secondaries，seconda想移除primary。在配置管理上，只有一个reconfiguration会成功，另一个会失败，因为成功的那个reconfiguration之后会增长version。这样就拒绝了另外一个（这其实就是在配置管理的paxos集群中进行一次表决，最终只有一个结果能够完成）。

任何故障检测机制只要能够保证Primary Invariant就可以用来触发移除当前replica。
Primary Invariant：任意时刻，一个replica，只有当cm认为其为primary时，这个replica才会认为自己是primary。因此，任意时刻只有一个leader。

## 2.3 Lease and Failure Detection
即使cm能够维护一份有效的配置，Primary Invariant也不一定能时刻保证成立。因为每个replica上的configuration不能保证时刻相同。
必须避免双主的产生：两个primary同时处理请求。比如，老的primary被网络分区了，它还没有意识到有新的primary产生，自己已经被移除出去。
双主的危害是：新的primary处理新的请求，同时client通过老的primary读取到老的数据，违背了强一致性。

我们的解决方式是通过lease机制。primary定期从secondaries上申请租约(承认我是primary的租约)。当所有的secondaries都发放了租约了，primary的租约就又被延长了，否则primary在一个租约时间没有收到所有的ack就主动降级。这样：
	1. 在lease_period之内，primary没有收到secondaries的租约回复，primary认为自己不是leader，primary降级后，同时通知cm移除这个replica；
	2. 在grace_period之内，secondaries没有收到primary的租约，同时secondaries记录的上一个primary的租约也过期了，那么secondaries就通知cm移除primary，同时自己成为primary。
可以看出，所有的secondaries都有机会给cm发送自己成为新的primary的机会，primary是通过竞争产生的，实质上就是一个上锁的过程。

只要grace_period <= lease_period，就能保证primary比secondaries先超时
我们使用租约机制做为故障检测机制，分布式系统中租约机制是常用手段，比如GFS，Boxwood，Bigtable/Chubby。不同点是，这几个系统的租约是从一个中心化的节点上申请租约，而Pacific-A是去中心化的。一个优化：
	1. 当primary和secondaries之间链路繁忙时，有update请求和ack时，无需再发送租约了；
	2. 当primary和secondaries之间链路idle时，才需要发送租约；

租约是双向的；租约是去中心化的；租约在idle时需要；
bsr的租约是单向的：
	1. leader开始给租约计时；
	2. leader给多数派申请租约；
	3. 成功后，多数派认可这个leader；
	4. 多数派选举出新leader；
	5. 这个新leader不能立即生效，因为老的leader可能还在。它的生效时间为上一个租约的过期时间，或者，固定的等待一个lease时间也可以保证老的leader失效；

## 2.4 Reconfiguration, Reconciliation, Recovery
一个复制协议的复杂度体现在reconfiguration的过程中。Pacific-A把reconfiguration分成3中类型：
	1. 移除secondaries；
	2. 移除primary；
	3. 添加secondaries；

Removal of Secondaries：当primary发现某个secondary故障了，primary就把这个secondary剔除出去。
	1. primary通过lease机制发现某个secondary故障；
	2. primary给cm发送新的configuration；
	3. 得到cm的同意后，primary剔除掉这个secondary；
	4. 继续服务（在剔除的过程中有可能仍然在给其他正常的secondary继续复制日志）；

Change of Primary： 通过lease机制secondary发现primary故障了，就触发移除primary的流程。secondary给cm发送移除primary的configuration，并且自己成为primary。这个过程称为reconfiguration。
	1. secondary通过lease机制发现primary故障；
	2. secondary给cm发送移除当前primary，并且自己成为新primary的请求；
	3. 得到cm的认可后；
	4. secondary成为新的primary；
	5. 开始进入reconciliation阶段：把prepaed-list上还没有commit的消息在新的configuration下走一遍复制协议，直到这些日志在新的configuration下都达到了commit状态；
	6. reconciliation之后，新的primary的新的commit-point指向了最新的已经commited的日志，但是其他secondary节点上的日志可能比自己的长很多，给secondaries发送truncate消息，参数就是自己的commted-point，指示其他节点的日志和自己对齐；
	7. 最终其他所有的secondary的prepared-list都和新的primary对齐了，但是committed-list不一定，因为刷committed-point是异步的逻辑，不要求每次有日志commit了都会刷新一次committed-point，但是新的primary在reconciliation结束后会强制刷一次，为了保证Commit/Reconfiguration Invariant；
	8. 这个过程也可能失败，再次触发reconfiguration；

Reconfiguration Invariant：新primary在完成了reconciliation之后，新的committed-list包含之前任意时刻任意secondary的committed-list，同时Commit Invarian成立；

Reconfiguration Invariant是对reconfiguration过程中Commit Invariant的扩展。从sn的角度看，任意被分配给commit的sn，后续都不会被重新使用。相反的，分配给没有uncommit的sn可能会被重新使用，比如新的primary的日志比较短，那么它可能truncate其他比较长的日志，也就是新primary上committed-point之后的sn在其他secondaries上被复用了。
secondaries总是接收最高verison号的primary的日志。

Addition of New Secondaries：当某些secondaries发生了故障，需要添加新的节点，维护预期的冗余级别。新的节点加入也需要满足Commit Invariant性质，也就是，新节点不能立即加入，提前从其他节点同步state，这个过程称为recovery。

新加入的secondary的状态为candidate，并不立即生效，需要等数据都同步完了后再正式生效。如何同步数据呢？
	1. 服务不中断：catch up的同时，primary处理，并且给c发送新的request；等c彻底的完成catch up，c向primary要求把自己添加到configuration中；
	2. 全量：新节点加入需要transfer全量Sate；
	3. 增量：老节点的rejoin，commit-list满足前缀关系；prepared-list不满足（期间发生了reconfiguration），那么需要截断，同时从primay 主动拉取缺失的prepared-list；

和polarfs的比较：
	1. 都有一个candidate状态；
	2. catchup的同时都不中断服务，一边同步一边接收新请求；
	3. 增量：pa需要主动trucate，主动拉取；
	4. 全量：全量同步全局state，和对应committed-point之后的日志；

## 2.5 Correctness
从以下3个方面论述协议的正确性。
Linearizability：committed-list上的update，和发生query的顺序是逻辑时间排序的，不能乱序。比如读取老的primary，但是此时新的primary已经更新了committed-list了，如果读取老的primary，query的顺序就不是在最新的committed-list之后了；
Durability：
Progress：

## 2.6 Implementations
在实践中，并不像上述描述的方式来维护prepaed-list和committed-list。最优的方案取决于业务data的语义，必须高效的实现下面3个操作：
	1. query processing：处理对已经committed-list的读取操作；
	2. State transfer during reconciliation：选出新的primary后的协商时，会把secondary上的日志进行truncate；
	3. State transfer during recovery：增量或全量传输secondary上缺失的prepared-list，同时，也可能会truncate操作；

把已经committed-list上日志按照sn号进行apply得到业务state，读取的时候直接读取这个state。没有commit的日志需要单独存储，这些日志可能会被truncate。
在reconciliation时，直接同步uncommitted的日志是简单的，因为这部分日志在prepared-list上，是被单独维护的。 <font color=red> 为了加速同步，可以把committ-list逻辑上切分成没n个落到一个state中，切分成一个个的segment。这个一个trade-off，即能定期回收已经committed的日志，也能给state保持一定的时间序列，给secondary同步的时候，需要对比segment列表，大概率是增量同步某些segment。</font>

上述的过程写放大是2次写，一次写prepared-list，一次是apply时写state。
<font color=red>Pacific-A的优化方案是：batch，先把日志在内存中apply，然后定期的写入磁盘。也即是已经commit的日志并不能立即被回收，committed-point不能往前移动，直到内存中的状态落盘成功，更新一次committed-list，此时才能出发回收；</font>

Design for Append-Only Application State：prepared-list可以直接作为application state，因为此时两个日志功能是等价的。

## 2.5 Discussion
Availabiliti and Performance of the configuration manager：中心化的cm简化了系统，同时简化数据复制链路的协议部分，n个副本能容忍n-1个故障。
cm是标准的paxos实现，5或7个副本，能容忍半数的节点故障。另外，cm的unavailable并不影响数据复制链路的可用性。

cm不会是性能瓶颈，因为数据复制链路并不经过cm。cm仅仅处理reconfiguration，cm根据local的数据可以比较version直接拒绝version的请求。

Durability during Transient Replica-Group Failure：如果所有replica都宕机了，则会出现data loss，如果只要有一个replica能启动起来，同时cm有多数派启动，那么就能恢复出完整的数据。
当一个节点重启后，可以向cm询问当前的configuration，自己是否还在Configuration中，并能得知自己之前是primary还是secondary：
	1. 如果是primary：立即社申请租约；
	2. 如果是secondary：响应beacon；

Primary/backup VS paxos：
	1.1 paxos是多数派协议，容忍少数慢节点，或者故障，不会影响整体的延迟；
	1.2 primary/back 是全同步，只要有一个慢，整体就慢，这个时候必须依赖cm及时的剔除掉故障节点，replica group才能恢复运行。另一方面，全同步的模式下，某个节点慢，整体就慢，慢节点和其他节点的差距也不会很大(只有一些uncommitted-list日志)。如果仅仅是网络闪断，cm把他剔除掉后面又join进来，可能是增量恢复。但是一进一出需要一个lease时间。对lease的设定要求比较严格，太长了影响故障发现的时间影响可用性；太短了可能会出现虚假的故障诊断；
	2.1 paxos，在多数派故障时不能再提供服务了，而primary/backup则可以通过reconfiguration，选出一个还活着的primary继续提供服务；
	3.借助cm，Pacific-A的变配比较直观；而paxos的变配需要通过自身的一致性协议，新加入的节点需要和多数派通信，以获得最新的state；
	
在工程实践中，哪种选择是比较优的tradeoff是有争议的。Pacific-A之所以选择primary/backup因为它很简洁。

Weakening Strong Consistency，强一致性有两个要点：
	1. 所有的replica以相同的顺序apply日志；
	2. 总能读取到最新的state；
适当的放松强一致性也许能得到其他优势，比如放松第一点，不要求所有的replica相同的顺序apply，那么replica之间的状态可能会出现分叉，这样就要求系统检测和恢复出一个一致的状态，同时要求client处理这种不一致。类似的系统有GFS，Coda。
Pacific-A放松要求的是第二点，允许读取都out-of-date state，意味着：
	1. 可以优化掉leader机制，这样Primary Invariant被打破，可能会出现双主，但是提高了可用性，新primary的选举无需等待lease过期。双primary不会出现state的分叉：因为新primary一定要在上一个Configuration中出现(上一个Configuration中的某个secondary发现primary没有租约申请了才发起的reconfiguration)，那么上一个的老的primary写入log就不会得到这个新primary的同意(它的version号比自己的低)；
	2. 另外，前天副本也可以提供读服务，commit invariant保证了secondary上的committed的日志比最新primary少，但是不会出现分叉；

# REPLICATION FOR DISTRIBUTED LOG-BASED STORAGE SYSTEMS
![pacifica-a-2]({{ site.url }}/images/2019-04-30-pacific-a-2.png)


1. 写log；
2. 更新内存；
3. 内存定期刷到checkpoint；
4. 已经写入checkpoint对应的日志，可以被truncate掉了；
5. checkpoint逐个被合并入 sate image；

把state分成一个个的checkpoint文件有助于join时catch-up。

## 3.1 logical replication
每个replica可以各自决定何时checkpoint，何时merge，因此物理上每个replica的state并不相同，这种复制称为逻辑复制；
应用层的数据或者log，可以直接应用Pacific-A协议的复制log，优化一次写；
每个checkpoint是一段(segment)日志对应的小状态，catch-up时决定改同步哪些segment。

A Variaation of Logical Replication：逻辑复制的问题是，primary和secondaries都要经历一次内存apply，checkpoint，merge，对cpu，memnory，IO是一种浪费。pacific-A提出一种logical-v复制：
	1. 只有primary节点会做apply，checkpoint，merge。secondary仅仅存储prepared-list日志，checkpoint可以从primary一段一段的拉取，根据拉取到checkpoint来truncate自己本地的日志；
	2. 优点是：secondary上节省了memory，cpu；
	2. 缺点是：对网络压力增打，同时，reconfiguration时新的primary选出来后需要replay产生最新的satate后才能服务；

## 3.2 Layered Replication

## 3.3 Log Merging

通常有多个replica group，而一个server给多个replica group提供服务。这样，一个server就会有多个日志文件了，就会出现写多个文件，那么就破坏了append-log的顺序性的优势。

可以把一个server上的多个日志合并成一个日志文件。

# EVALUATION

# 5. RELATED WORKS

GFS在有failover发生时，是弱一致性，不进行reconfiguration，reconciliation，增加了client的复杂度。
Bigtable是在GFS上的表格系统，通过chubby来决定当前table是由谁来master(有写入的权利)，co-location是困难的。


# 6. CONCLUDING REMARKS
























